\documentclass[a4paper,11pt,notitlepage,fullpage]{article}
%\documentclass{report}

\usepackage{fullpage}
\usepackage[utf8]{inputenc}
\usepackage[ngerman]{babel}
%\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{latexsym}
\usepackage{mathtools}
\usepackage{listings}
\usepackage{bbm}
%\usepackage{algorithm}
%\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hhline}
\usepackage{amsthm}
\usepackage{cite}
\usepackage{wrapfig}
\usepackage{hyperref}
\usepackage{titling}
\usepackage{color}

\setlength{\droptitle}{-60pt}

\newcommand{\R}{\mathbb R}
\newcommand{\p}{\mathbb P}
\newcommand{\pp}[1]{\mathbb P\left[#1\right]}
\newcommand{\E}{\mathbb E}
\newcommand{\Ee}[1]{\mathbb E\left[#1\right]}
\newcommand{\V}{\mathbb V}
\newcommand{\Vv}[1]{\mathbb V\left[#1\right]}
\newcommand{\Cov}[1]{\mathbb Cov\left[#1\right]}
\newcommand{\F}{\mathcal{F}}
\newcommand{\ind}{\mathbbm{1}}
\newcommand{\indd}[1]{\mathbbm{1}_{#1}}

\begin{document}
\author{Florian Bogner \& Alexander Palmrich}
\title{Stochastische Prozesse - Übung 6}
\maketitle

\begin{enumerate}
\setcounter{enumi}{24}

%%für ein Bild das copy-pasten und reinkommentieren
%\begin{figure}[h!]
%\centering
%\includegraphics[width=0.9\textwidth]{gfx/bildname.png}
%\label{fig1}
%\caption{TODO Beschreibung des Bildes}
%\end{figure}

%01
\item bla
\begin{enumerate}
%a
\item a
\begin{align*}
\end{align*}

%b
\item b
\begin{align*}
\end{align*}
\end{enumerate}


%02
\item bla
\begin{enumerate}
%a
\item a
\begin{align*}
\end{align*}

%b
\item b
\begin{align*}
\end{align*}
\end{enumerate}

%03
\item Wir nummerieren die Räume von 1 bis 6 zeilenweise, i.e. der Start ist 1, der Käse ist 5 und die Falle ist 6. Die Anfangsverteilung lautet $\lambda = (1,0,0,0,0,0)$ und die Übergangsmatrix lautet dann
$$P = \begin{pmatrix}
&\frac{1}{2}&\frac{1}{2}&&& \\
\frac{1}{2}&&&\frac{1}{2}&& \\
\frac{1}{3}&&&\frac{1}{3}&\frac{1}{3}& \\
&\frac{1}{3}&\frac{1}{3}&&&\frac{1}{3} \\
&&&&1& \\
&&&&&1 
\end{pmatrix}$$
In der Sprache der Markovketten sind Zustände 5 und 6 absorbierend. Die anderen Zustände sind transient. Wir suchen die Absorbtionswahrscheinlichkeit des Zustands 6 unter der gegebenen Anfangsverteilung.
Sei $p_i$ die Absorbtionswahrscheinlichkeit des Zustands 6, wenn man in Zustand $i$ startet. Offensichtlich gilt $p_5 = 0$ und $p_6 = 1$. Die anderen $p_i$ erfüllen ein Gleichungsystem. Die Absorbtionswahrscheinlichkeit ist der nach Übergangswahrscheinlichkeiten gewichtete Durchschnitt der anderen Absorbtionswahrscheinlichkeiten.
$$p_i = \sum_j P_{ij} p_j$$
Außerdem, aufgrund der Symmetrie in dem Irrgarten gilt $p_1 = 1 - p_2$ und $p_3 = 1-p_4$. Zusammengefasst und eingesetzt bekommt man die zwei Gleichungen:
\begin{align*}
p_1 &= \frac{1}{2}(1 - p_1 + p_3) \\
p_3 &= \frac{1}{3}(p_1 + 1 - p_3 + 0)
\end{align*}
Umgeformt:
\begin{align*}
3 p_1 - 1&= p_3 \\
- p_1 + 4 p_3 &= 1
\end{align*}
Erste Gleichung in zweite eingesetzt ergibt:
$$- p_1 + 4 (3 p_1 - 1) = 1$$
$$11 p_1 - 4 = 1$$
$$p_1 = \frac{5}{11}$$
Wir müssen $p_3$ gar nicht mehr berechnen, da $p_1$ gleich unsere Antwort ist: Die Maus tappt mit Wahrscheinlichtkeit $\frac{5}{11}$ in die Falle. Armes Mausi.

%04
\item Mit dem Satz der Totalen Wahrscheinlichkeit und der Eigenschaft strikt stationär geht das ganz ez:
\begin{align*}
\lambda_0 &= \pp{X_0 = 0} \\
&= \pp{X_0 = 0, X_1 = 0} + \pp{X_0 = 0, X_1 = 1} \\
&= 0.25+0.15 = 0.4 \\
\lambda_1 &= 1-\lambda_0 = 0.6 \\
P_{00} &= \pp{X_{n+1} = 0 | X_n = 0} \\
&= \pp{X_{n+1} = 0 | X_n = 0, X_{n-1} = 0}\cdot\pp{X_{n-1} = 0} + \cdots \\
&~~~~\cdots + \pp{X_{n+1} = 0 | X_n = 0, X_{n-1} = 1}\cdot\pp{X_{n-1} = 1} \\
&= 0.7 \cdot \lambda_0 + 0.5 \cdot \lambda_1 \\
&= 0.58 \\
P_{10} &= \pp{X_{n+1} = 0 | X_n = 1} \\
&= \pp{X_{n+1} = 1 | X_n = 1, X_{n-1} = 0}\cdot\pp{X_{n-1} = 0} + \cdots \\
&~~~~\cdots + \pp{X_{n+1} = 0 | X_n = 1, X_{n-1} = 1}\cdot\pp{X_{n-1} = 1} \\
&= 0.4 \cdot \lambda_0 + 0.2 \cdot \lambda_1 \\
&= 0.28 \\
P_{01} &= 1 - P_{00} = 0.42 \\
P_{11} &= 1 - P_{10} = 0.72
\end{align*}
TODO: Überlegen ob das richtig ist. Kann man aus der strikt stationär Eigenschaft wirklich schließen, dass die unbedingte Verteilung die gleiche ist wie die Startverteilung? Außerdem muss man eigentlich zeigen, dass X strikt stationär ist.

Laut Markov-Eigenschaft müsste dann ja z.B. $$\pp{X_{n+1} = 0 | X_n = 0} = \pp{X_{n+1} = 0 | X_n = 0, X_{n-1} = 0}$$ gelten, aber klarerweise ist $0.58 \neq 0.7$.

Wir definieren uns eine Markovkette: Als Zustandsraum $S := \{00, 10, 01, 11\}$ wählen wir Zusammenfassungen von je zwei Tagen. Die Anfangsverteilung $\lambda := (0.25, 0.15, 0.15, 0.45)$ lesen wir direkt aus der Tabelle ab. Die Berechnung der Übergangswahrscheinlichkeiten ist einen Hauch komplizierter. Sei $a, b, c, d \in \{0, 1\}$
\begin{align*}
P_{ab, cd} &= \pp{X_{n+2} = d, X_{n+1} = c | X_n = b, X_{n-1} = a} \\
&= \pp{X_{n+1} = c | X_n = b, X_{n-1} = a} \cdot \pp{X_{n+2} = d | X_{n+1} = c, X_n = b, X_{n-1} = a} \\
&= \pp{X_{n+1} = c | X_n = b, X_{n-1} = a} \cdot \pp{X_{n+2} = d | X_{n+1} = c, X_n = b}
\end{align*}
Die vorkommenden Wahrscheinlichkeiten sind in der Tabelle zu finden. Wir wir rechenen illustrativ $P_{01,10}$ aus:
\begin{align*}
P_{01, 10} &= (1-0.4) \cdot 0.2 = 0.12
\end{align*}

%05
\item
\begin{figure}[h!]
\centering
\includegraphics[width=0.5\textwidth]{gfx/29_a.pdf}
\caption{\label{fig:graph} Graph der Markovkette in 29 (a)}
\end{figure}

\begin{enumerate}
%a
\item Die Matrix lesbar aufgeschrieben lautet
\begin{align*}
P = \frac{1}{10}\begin{pmatrix}
&1&9&&&&&\\
1&9&&&&&&\\
&&6&2&&2&&\\
&&&5&5&&&\\
&&&&&4&3&3\\
&&&&10&&&\\
&&&&&&9&1\\
&&&&&&&10
\end{pmatrix}
\end{align*}
In Abbildung \ref{fig:graph} sehen wir eine graphische Representation der Markovkette. Nur positive Übergangswahrscheinlichkeiten sind eingezeichnet. Die einzige nichttriviale Kommunikationsklasse $\{1, 2\}$ ist strichliert eingezeichnet. Alle anderen Kommunikationsklassen enthalten nur einen Zustand. Damit ist die Markovkette reduzibel.
%b
%copypasta : P = np.array([[int(d) for d in line] for line in "0110 1001 0011 0011".split(" ")])/2
\item Behauptung für $n \geq 1$:
\begin{align*}
n~\text{gerade} &\Rightarrow P^n = \begin{pmatrix}
\frac{1}{2}^n&0&\frac{1}{2}-\frac{1}{2}^n&\frac{1}{2} \\
0&\frac{1}{2}^n&\frac{1}{2}&\frac{1}{2}-\frac{1}{2}^n \\
0&0&\frac{1}{2}&\frac{1}{2}\\
0&0&\frac{1}{2}&\frac{1}{2}
\end{pmatrix} \\
n~\text{ungerade} &\Rightarrow P^n = \begin{pmatrix}
0&\frac{1}{2}^n&\frac{1}{2}&\frac{1}{2}-\frac{1}{2}^n \\
\frac{1}{2}^n&0&\frac{1}{2}-\frac{1}{2}^n&\frac{1}{2} \\
0&0&\frac{1}{2}&\frac{1}{2}\\
0&0&\frac{1}{2}&\frac{1}{2}
\end{pmatrix} 
\end{align*}

Beweis per Induktion: Induktionsanfang \emph{trivial}.

Gerader Induktionsschritt:
\begin{gather*}
P^n = P^{n-1} \cdot P = \begin{pmatrix}
0&\frac{1}{2}^{n-1}&\frac{1}{2}&\frac{1}{2}-\frac{1}{2}^{n-1} \\
\frac{1}{2}^{n-1}&0&\frac{1}{2}-\frac{1}{2}^{n-1}&\frac{1}{2} \\
0&0&\frac{1}{2}&\frac{1}{2}\\
0&0&\frac{1}{2}&\frac{1}{2}
\end{pmatrix} \cdot \begin{pmatrix}
0&\frac{1}{2}&\frac{1}{2}&0 \\
\frac{1}{2}&0&0&\frac{1}{2} \\
0&0&\frac{1}{2}&\frac{1}{2}\\
0&0&\frac{1}{2}&\frac{1}{2}
\end{pmatrix} \\
= \begin{pmatrix}
\frac{1}{2}^{n-1} \cdot \frac{1}{2} & 0 & \frac{1}{2}\cdot\frac{1}{2} + (\frac{1}{2} - \frac{1}{2}^{n-1})\cdot\frac{1}{2} & \frac{1}{2}^{n-1} \cdot \frac{1}{2} + \frac{1}{2}\cdot\frac{1}{2} +  (\frac{1}{2} - \frac{1}{2}^{n-1})\cdot\frac{1}{2}\\
0 & \frac{1}{2}^{n-1} \cdot \frac{1}{2} & \frac{1}{2}^{n-1} \cdot \frac{1}{2} + \frac{1}{2}\cdot\frac{1}{2} +  (\frac{1}{2} - \frac{1}{2}^{n-1})\cdot\frac{1}{2} & \frac{1}{2}\cdot\frac{1}{2} + (\frac{1}{2} - \frac{1}{2}^{n-1})\cdot\frac{1}{2} \\
0&0&\frac{1}{2}\cdot\frac{1}{2} + \frac{1}{2}\cdot\frac{1}{2}&\frac{1}{2}\cdot\frac{1}{2} + \frac{1}{2}\cdot\frac{1}{2}\\
0&0&\frac{1}{2}\cdot\frac{1}{2} + \frac{1}{2}\cdot\frac{1}{2}&\frac{1}{2}\cdot\frac{1}{2} + \frac{1}{2}\cdot\frac{1}{2}
\end{pmatrix} \\
= \begin{pmatrix}
\frac{1}{2}^n&0&\frac{1}{2}-\frac{1}{2}^n&\frac{1}{2} \\
0&\frac{1}{2}^n&\frac{1}{2}&\frac{1}{2}-\frac{1}{2}^n \\
0&0&\frac{1}{2}&\frac{1}{2}\\
0&0&\frac{1}{2}&\frac{1}{2}
\end{pmatrix}
\end{gather*}

Ungerader Induktionsschritt analog:
\begin{gather*}
P^n = P^{n-1} \cdot P = \begin{pmatrix}
\frac{1}{2}^{n-1}&0&\frac{1}{2}-\frac{1}{2}^{n-1}&\frac{1}{2} \\
0&\frac{1}{2}^{n-1}&\frac{1}{2}&\frac{1}{2}-\frac{1}{2}^{n-1} \\
0&0&\frac{1}{2}&\frac{1}{2}\\
0&0&\frac{1}{2}&\frac{1}{2}
\end{pmatrix} \cdot \begin{pmatrix}
0&\frac{1}{2}&\frac{1}{2}&0 \\
\frac{1}{2}&0&0&\frac{1}{2} \\
0&0&\frac{1}{2}&\frac{1}{2}\\
0&0&\frac{1}{2}&\frac{1}{2}
\end{pmatrix} \\
= \begin{pmatrix}
0 & \frac{1}{2}^{n-1} \cdot \frac{1}{2} & \frac{1}{2}^{n-1} \cdot \frac{1}{2} + \frac{1}{2}\cdot\frac{1}{2} +  (\frac{1}{2} - \frac{1}{2}^{n-1})\cdot\frac{1}{2} & \frac{1}{2}\cdot\frac{1}{2} + (\frac{1}{2} - \frac{1}{2}^{n-1})\cdot\frac{1}{2} \\
\frac{1}{2}^{n-1} \cdot \frac{1}{2} & 0 & \frac{1}{2}\cdot\frac{1}{2} + (\frac{1}{2} - \frac{1}{2}^{n-1})\cdot\frac{1}{2} & \frac{1}{2}^{n-1} \cdot \frac{1}{2} + \frac{1}{2}\cdot\frac{1}{2} +  (\frac{1}{2} - \frac{1}{2}^{n-1})\cdot\frac{1}{2}\\
0&0&\frac{1}{2}\cdot\frac{1}{2} + \frac{1}{2}\cdot\frac{1}{2}&\frac{1}{2}\cdot\frac{1}{2} + \frac{1}{2}\cdot\frac{1}{2}\\
0&0&\frac{1}{2}\cdot\frac{1}{2} + \frac{1}{2}\cdot\frac{1}{2}&\frac{1}{2}\cdot\frac{1}{2} + \frac{1}{2}\cdot\frac{1}{2}
\end{pmatrix} \\
= \begin{pmatrix}
0&\frac{1}{2}^n&\frac{1}{2}&\frac{1}{2}-\frac{1}{2}^n \\
\frac{1}{2}^n&0&\frac{1}{2}-\frac{1}{2}^n&\frac{1}{2} \\
0&0&\frac{1}{2}&\frac{1}{2}\\
0&0&\frac{1}{2}&\frac{1}{2}
\end{pmatrix}
\end{gather*}
\qed

Im Grenzwert:
\begin{align*}
P^\infty = \begin{pmatrix}
0&0&\frac{1}{2}&\frac{1}{2}\\
0&0&\frac{1}{2}&\frac{1}{2}\\
0&0&\frac{1}{2}&\frac{1}{2}\\
0&0&\frac{1}{2}&\frac{1}{2}
\end{pmatrix}
\end{align*}

Daraus sieht man die Zustände 1 und 2 sind gemeinsam in einer Kommunikationsklasse transient und analog sind die Zustände 3 und 4 in einer Kommunikationsklasse rekurrent.
\end{enumerate}

\end{enumerate}



\end{document}
